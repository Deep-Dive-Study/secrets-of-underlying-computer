# 5장 작은 것으로 큰 성과 이루기, 캐시

폰 노이만 구조는 명령어와 명령어가 사용하는 데이터가 메모리에 저장되어야 하며, CPU가 기계 명령어를 실행할 때 먼저 명령어를 메모리에서 읽어야 한다. 또한, 연산이 실행된 이후에도 메모리에 저장해야 한다.

# 5.1 캐시, 어디에나 존재하는 것

### CPU와 메모리의 속도 차이

- CPU는 빠르게 연산하지만, 메모리는 상대적으로 느려서 속도 차이가 발생함
- 메모리 속도가 CPU 속도를 따라가지 못하기 때문에 CPU는 항상 데이터를 기다려야 하는 상태가 됨
- 이를 해결하기 위해 캐시(cache) 가 도입됨

**CPU와 메모리 속도 비교**

| 연도 | CPU 성능 증가율 | 메모리 속도 증가율 |
| --- | --- | --- |
| 1980 | 10배 | 2배 |
| 2000 | 100배 | 5배 |
| 2020 | 1000배 이상 | 10배 내외 |

### 도서관, 책상, 캐시

- 도서관(메모리)에서 책을 가져와 책상(캐시)에 올려두면 빠르게 참고할 수 있음
- 컴퓨터도 자주 사용하는 데이터를 캐시에 저장해 메모리에 접근하는 속도를 줄임

**캐시 계층 구조**

1. L1 캐시: 가장 빠르지만 용량이 작음
2. L2 캐시: L1보다 느리지만 용량이 더 큼
3. L3 캐시: L2보다 더 크지만 속도가 더 느림
- CPU는 데이터를 읽을 때 캐시에서 먼저 찾고, 없으면 메모리에 접근함
- L1, L2, L3 캐시는 모두 레지스터 칩 내에 묶여 패키징 되어 있다.

### 공짜 점심은 없다 : 캐시 갱신

- 캐시에 저장된 데이터가 메모리와 일치하지 않는 불일치 문제 발생
- 이를 해결하는 두 가지 방식
    - 연속 기입(write-through): 캐시가 갱신될 때마다 메모리도 함께 갱신
    - 후기입(write-back): 캐시만 먼저 갱신하고, 나중에 메모리를 갱신

문제점

- write-through 방식은 메모리를 자주 업데이트해야 해서 성능 저하
- write-back 방식은 속도는 빠르지만 데이터가 유실될 가능성이 있음

### 공짜 저녁은 없다 : 다중 코어 캐시의 일관성

- 멀티코어 CPU에서는 각 코어마다 캐시가 따로 존재함
- 같은 변수를 여러 캐시에 저장하면 값이 다를 수 있음

**캐시 일관성 유지**

1. MESI 프로토콜
    - Modified, Exclusive, Shared, Invalid 상태를 이용해 캐시 동기화
2. 캐시 코히어런시(Coherency) 기술
    - 캐시에 저장된 데이터를 다른 코어가 변경하면 이를 감지하고 갱신

### 메모리를 디스크의 캐시로 활용하기

- 메모리는 디스크보다 10만 배 이상 빠름
- 디스크 입출력 속도를 높이기 위해 메모리를 캐시로 사용
- 최근에는 RAM이 디스크를 대체하는게 대세

운영체제의 페이지 캐시

- 운영체제는 남는 메모리를 파일 캐시 로 활용하여 디스크 접근을 줄임
- 파일을 처음 열 때는 느리지만, 두 번째 열 때는 캐시에서 가져와 빠름

### 가상 메모리와 디스크

가상 메모리의 개념

- 실제 메모리보다 큰 주소 공간을 제공하기 위해 디스크를 활용
- 메모리 공간에 여유가 없을 때 잘 사용되지 않는 데이터를 디스크로 옮기고, 필요할 때 다시 불러옴

페이지 폴트(Page Fault)

- 실행 중인 프로그램이 필요한 데이터가 메모리에 없을 때 발생
- 운영체제가 디스크에서 해당 데이터를 불러오지만, 속도가 매우 느림

### CPU는 어떻게 메모리를 읽을까?

메모리 접근 과정

1. CPU는 가상 주소를 물리 주소로 변환
2. 캐시에서 데이터를 검색
3. 캐시에 없으면 메모리에서 검색
4. 메모리에도 없으면 디스크에서 데이터를 불러와 메모리에 저장 후 사용

이 과정에서 캐시 적중률이 높을수록 속도가 빨라짐

### 분산 저장 지원

분산 저장 시스템 : 장치 여러 대를 사용한다.

- 하나의 디스크로 데이터를 저장할 수 없을 때 여러 개의 디스크를 이용
- 데이터베이스, 클라우드 시스템에서 많이 사용됨 (카프카)

계층적 캐시 구조

- 레지스터 → L1~3 캐시 → 메모리 → 디스크 → 분산 파일 시스템 순으로 속도가 느려짐
- 각 계층이 다음 계층을 위한 캐시 역할을 수행
- 크기도 순차적이어야 한다.

# 5.2 어떻게 캐시 친화적인 프로그램을 작성할까?

### 프로그램 지역성의 원칙

시간적 지역성

- 프로그램이 메모리 조작에 접근하고 그 조각을 여러 번 참조하는 경우
- 캐시 친화성이 매우 높음
- 캐시에 있는 한 메모리에 접근하지 않아도 반복적으로 캐시 적중이 가능

공간적 지역성

- 인접한 메모리를 참조하는 경우
- 캐시 친화적이다.
- 캐시에 적중하지 못하면 메모리의 데이터를 캐시에 옮기는데 이 때 같이 옮겨짐

### 메모리 풀 사용

메모리를 동적으로 할당받을 때 보통 malloc을 사용하는데 공간적 지역성이 좋지않다.

메모리 풀

- 메모리 풀은 고성능을 요구할 때만 사용한다.
- malloc을 사용하지 않는다.
- 초기화 할 때 일반적으로 연속된 메모리 공간을 할당받는다. (공간적 지역성 굿)

### struct 구조체 재배치

연결 리스트 탐색하는 프로그램

```c
#define SIZE 10000

struct List {
	List* next;
	int arr[SIZE];
	int value;
};

bool find(struct List* list, int target) {
	while(list) {
		if(list->value == target) {
			return true;
		}
		list = list->next;
	}
	return false;
}
```

위 코드에서 next와 value는 빈번하게 사용되며, arr는 전혀 사용되지 않는다.

하지만 arr에 의해 next와 value 값이 멀리 떨어져 있기 때문에 공간적 지역성이 나빠질 수 있다.

→ next 포인터와 value 값을 함께 배치하는 것이 효율적이다.

```python
#define SIZE 10000
struct List {
	List* next;
	int value;
	int arr[SIZE];
};
```

### 핫 데이터와 콜드 데이터의 분리

연결 리스트에 노드가 많을 때 캐시 공간을 많이 차지하기 때문에 배열 arr를 다른 구조체에 넣고 List 구조체 안에 이 구조체를 가리키는 포인터를 추가한다.

```c
struct List {
	List* next;
	int value;
	struct Arr* arr;
};

struct Arr {
	int arr[SIZE];
};
```

이러면 List의 크기는 더욱 줄어들고, 캐시는 더 많은 노드를 저장할 수 있다.

arr → 콜드데이터

next, value → 핫데이터

### 캐시 친화적인 데이터 구조

- vector가 list보다 더 지역성이 좋다.
- 연결 리스트의 자유로움을 유지하면서 캐시 친화적이고 싶다면 메모리 풀을 사용하라

### 다차원 배열 순회

1. 캐시가 비어있으니 A0을 읽으며 4개의 데이터 적재
    
    ![image](https://github.com/user-attachments/assets/f5b2e979-fff9-4165-8186-b827e9854e68)
    
2. A4를 읽을 때 캐시 hit을 못해서 메모리로부터 읽어옴
    
    ![image](https://github.com/user-attachments/assets/24143dcc-63e6-4c23-bf41-02ae643368b4)
    

이때 캐시 적중률은 75%

행 방식이 아닌 열 방식으로 접근하게 되면 캐시 적중률은 0%가 된다.

**반드시 성능 분석 도구를 사용하여 캐시 정중률이 시스템 성능에 병목이 되는지 판단해야 한다.**

# 5.3 다중 스레드 성능 방해자

### 캐시 라인

- 캐시는 개별적인 데이터를 저장하는 것이 아니라 일정 크기의 블록(캐시 라인) 단위로 데이터를 저장
- 일반적으로 캐시 라인 크기는 64바이트

**캐시 라인의 역할**

1. CPU는 데이터를 불러올 때 64바이트 단위로 가져옴
2. 인접한 데이터를 한꺼번에 로드하여 메모리 접근 속도를 최적화
3. 캐시 적중률을 높이면 CPU 성능이 향상됨

| 주소 | 데이터 | 캐시 라인 |
| --- | --- | --- |
| 0x1000 | A | 캐시 라인 1 |
| 0x1004 | B | 캐시 라인 1 |
| 0x1008 | C | 캐시 라인 1 |
| 0x1010 | D | 캐시 라인 2 |
| 0x1014 | E | 캐시 라인 2 |
- A~C는 같은 캐시 라인에 포함되므로 한 번에 로드됨
- D~E는 다른 캐시 라인에 속해 별도로 로드됨

## 5.3.2 캐시 튕김 문제(Cache Bouncing)

- 두 개 이상의 스레드가 같은 캐시 라인을 번갈아가며 수정하면 불필요한 캐시 동기화가 발생하는 현상
- 캐시 일관성(Coherency) 유지 비용이 증가하여 성능이 급격히 저하됨

![image](https://github.com/user-attachments/assets/b1e512cd-3592-4756-a626-86e31e51274a)

발생 과정

1. 스레드 A가 변수 X를 수정 → 코어 A의 캐시에서 수정됨
2. 스레드 B가 같은 변수 X를 수정 → 코어 B의 캐시에서 변경됨
3. 코어 A의 캐시는 무효화됨 → 다시 X를 읽어야 함
4. 이러한 과정이 반복되면서 CPU 성능이 급격히 저하됨

해결 방법

1. 캐시 라인 분리: 공유 데이터가 불필요한 캐시 동기화를 유발하지 않도록 조정
2. 락을 최소화: 동기화 비용을 줄여 캐시 튕김 현상을 완화

### 거짓 공유 문제(False Sharing)

- 서로 다른 변수를 사용하지만, 같은 캐시 라인에 존재하여 불필요한 캐시 동기화가 발생하는 문제

![image](https://github.com/user-attachments/assets/2be2f6ab-e227-42f7-a8f3-a71756cb088f)

발생 과정

1. 변수 A와 B가 같은 캐시 라인에 저장됨
2. 스레드 A가 변수 A를 변경 → 해당 캐시 라인이 수정됨
3. 스레드 B가 변수 B를 사용하려고 할 때, 캐시가 무효화됨
4. 스레드 B가 다시 캐시를 로드해야 하므로 불필요한 메모리 접근 발생

해결 방법

1. 변수 간격을 조정하여 캐시 라인을 분리
2. 패딩(Padding)을 추가하여 캐시 충돌 방지
3. 데이터를 구조체로 묶어 캐시 최적화

### 결론

1. 캐시 라인은 CPU 성능 최적화를 위해 중요한 역할을 함
2. 캐시 튕김 문제를 방지하려면 공유 데이터를 최소화해야 함
3. 거짓 공유 문제를 해결하려면 변수 간격을 조정하여 캐시 라인을 분리해야 함

# 5.4 메모리 장벽과 다중 스레드 프로그래밍

### 메모리 장벽

다중 스레드 환경에서 발생하는 데이터 순서 문제를 해결하기 위한 도구

CPU나 컴파일러가 성능 향상을 위해 명령어 순서를 바꾸면서 예상치 못한 결과가 발생할 수 있는데, 이를 방지한다.

![image](https://github.com/user-attachments/assets/048c13e1-ec2b-4e17-a4ce-f3f6748b3b67)

왜 필요한가

- CPU가 성능을 위해 명령어 순서를 바꿀 수 있음
- 컴파일러도 최적화 과정에서 코드 순서를 재배치할 수 있음
- 멀티스레드 환경에서 데이터 일관성 보장이 필요함

### 메모리 장벽

- CPU가 명령어를 재정렬하지 못하도록 하는 기계 명령어

| 유형 | 설명 | 예시 |
| --- | --- | --- |
| LoadLoad | 읽기-읽기 순서 보장 | `a = x; b = y;` |
| LoadStore | 읽기-쓰기 순서 보장 | `a = x; y = b;` |
| StoreStore | 쓰기-쓰기 순서 보장 | `x = a; y = b;` |
| StoreLoad | 쓰기-읽기 순서 보장 | `x = a; b = y;` |

### 획득-해제(Acquire-Release)

메모리 장벽을 쉽게 사용할 수 있는 방법

- 획득(Acquire): 특정 Load 이전의 모든 메모리 작업이 완료되도록 강제
- 해제(Release): 특정 Store 이후의 모든 메모리 작업이 완료되도록 강제

**C++ 구현 예제**

```cpp
std::atomic_thread_fence(std::memory_order_acquire);
std::atomic_thread_fence(std::memory_order_release);
```

### CPU별 차이점

| 아키텍처 | LoadLoad | StoreStore | LoadStore | StoreLoad |
| --- | --- | --- | --- | --- |
| x86 | X | X | X | O |
| ARM, Power | O | O | O | O |
- x86: 대부분의 재정렬 금지 (안전함)
- ARM/POWER: 대부분의 재정렬 허용 (주의 필요)

### 잠금 없는 프로그래밍 (Lock-Free Programming)

- 공유 리소스에 대한 동기화를 락 없이 해결
- `CAS(Compare-And-Swap)` 같은 원자적 연산 활용
- 실시간 시스템에서 유용하지만 코드가 복잡해짐

| 방식 | 장점 | 단점 |
| --- | --- | --- |
| 잠금 (Lock) | 코드 구현이 쉬움, 명령어 재정렬 문제 없음 | 동시성 저하, 대기 시간 증가 |
| 잠금 없음 (Lock-Free) | 높은 동시성, 대기 시간 최소화 | 구현이 복잡, ABA 문제 발생 가능 |

### 결론

1. CPU는 성능 최적화를 위해 명령어를 재정렬하여 실행
2. 다중 스레드 환경에서는 메모리 장벽과 획득-해제 의미론을 활용해 동기화 필요
3. CPU 아키텍처별로 메모리 모델이 다르므로, 다중 플랫폼 지원 시 주의
4. 잠금 없는 프로그래밍은 높은 동시성을 제공하지만, 구현 난이도가 높음

# 5.5 요약

캐시는 기능을 위한 것이 아니라 성능을 위한 것이다.

1. 캐시 용량은 제한되어 있으므로 프로그램에 필요한 데이터에 집중해야 한다.
2. 여러 스레드 사이에 캐시 일관성을 유지해야하면 캐시 튕김 문제를 경계해야 한다.
